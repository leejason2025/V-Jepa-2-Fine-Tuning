# V-JEPA2-AC LoRA Fine-tuning Configuration - DEBUG MODE
# For quick testing on droid_100 (100 episodes)

# Model Configuration
model:
  predictor_path: null  # Path to V-JEPA2-AC predictor checkpoint (set when available)
  encoder_path: "pretrained_models/vjepa2-vitl"  # Path to V-JEPA2 encoder checkpoint
  freeze_encoder: true  # Keep encoder frozen, only train predictor

  # Architecture specs (300M predictor)
  num_layers: 24
  num_heads: 16
  hidden_dim: 1024
  activation: "gelu"

# LoRA Configuration
lora:
  enabled: true
  r: 16  # Rank (16-32 recommended for 305M model)
  lora_alpha: 32  # Scaling factor
  lora_dropout: 0.05
  use_rslora: true  # Rank-stabilized LoRA (alpha/sqrt(r))
  target_modules:
    - "attn.qkv"
    - "attn.proj"
    - "mlp.fc1"
    - "mlp.fc2"
  bias: "none"

# Data Configuration
data:
  dataset: "droid"
  bucket_name: "gresearch"
  bucket_prefix: "robotics"
  debug_mode: true  # Use droid_100 (100 episodes)

  # Video parameters
  video_resolution: 256
  fps: 4
  clip_length_sec: 2  # 2 seconds (was 4)
  frames_per_clip: 8  # 8 frames to match V-JEPA2-AC pretrained (was 16)

  # Camera view
  camera_view: "wrist"  # "wrist", "exterior_1", "exterior_2"

  # Processing - MINIMAL for quick testing
  filter_idle_actions: false  # Disabled for faster testing
  idle_threshold: 0.01
  clip_stride: 4  # Stride for sliding window (50% overlap with 8-frame clips)
  shuffle_buffer_size: 10  # Minimal buffer (was 100000)
  num_workers: 4
  prefetch_factor: 2

# Training Configuration
training:
  # Batch sizes
  per_device_batch_size: 2  # Max that fits in 24GB VRAM
  gradient_accumulation_steps: 16  # Effective batch = 2 * 16 = 32

  # Optimization
  learning_rate: 2.0e-5  # Reduced from 1e-4
  weight_decay: 0.01
  warmup_steps: 10  # Reduced for quick testing (was 250)
  max_steps: 50  # Just 50 steps for quick validation

  # Loss configuration
  teacher_forcing_steps: 15  # T=15 for teacher-forcing
  rollout_steps: 2  # T=2 for rollout loss
  loss_weight_tf: 1.0  # Teacher-forcing loss weight
  loss_weight_rollout: 1.0  # Rollout loss weight

  # Memory optimization
  gradient_checkpointing: true
  mixed_precision: "bf16"  # "fp16" or "bf16"
  use_8bit_adam: true

  # Logging
  log_every_n_steps: 5  # More frequent for quick testing
  eval_every_n_steps: 25  # Eval twice during test
  save_every_n_steps: 25  # Save twice during test

  # Checkpointing
  save_total_limit: 2  # Keep only last 2 checkpoints
  checkpoint_dir: "./checkpoints_debug"

  # Distributed training
  ddp_backend: "nccl"
  gradient_clipping: 1.0

# Evaluation Configuration
eval:
  num_eval_episodes: 10  # Much smaller for quick testing
  eval_batch_size: 1

# Weights & Biases
wandb:
  enabled: false  # Disabled for debug runs
  project: "vjepa2-ac-finetune-debug"
  entity: null
  run_name: null

# GCP Configuration
gcp:
  project_name: null
  checkpoint_bucket: null
  use_gce_credentials: true
